{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 安装和导入必要的库\n",
    "!pip install  unsloth"
   ],
   "id": "f6f165dfd27febfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n"
   ],
   "id": "b8a6c1749fe23a16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 定义KL散度计算函数 (这里是偏向前KL散度)\n",
    "def compute_skewed_fkl(logits_student, logits_teacher, target_labels, padding_id,\n",
    "                       reduction=\"sum\", temp=1.0, skew_lambda=0.1):\n",
    "    \"\"\"计算偏向前KL散度: KL(teacher || mixed_distribution)\n",
    "       mixed_distribution = skew_lambda * teacher + (1-skew_lambda) * student\n",
    "    \"\"\"\n",
    "    # 温度缩放\n",
    "    logits_student_scaled = logits_student / temp\n",
    "    logits_teacher_scaled = logits_teacher / temp\n",
    "\n",
    "    # 学生模型的概率 (来自缩放后的logits)\n",
    "    probs_student = torch.softmax(logits_student_scaled, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # 教师模型的概率和对数概率 (来自缩放后的logits, 不应反向传播梯度)\n",
    "    with torch.no_grad():\n",
    "        probs_teacher = torch.softmax(logits_teacher_scaled, dim=-1, dtype=torch.float32)\n",
    "        log_probs_teacher = torch.log_softmax(logits_teacher_scaled, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # 计算混合概率分布\n",
    "    # mixed_probs = skew_lambda * p_teacher + (1 - skew_lambda) * p_student\n",
    "    mixed_probs = skew_lambda * probs_teacher + (1 - skew_lambda) * probs_student\n",
    "    # 防止 mixed_probs 为0导致log(0)数值问题，添加一个极小值\n",
    "    mixed_log_probs = torch.log(mixed_probs + 1e-10)\n",
    "\n",
    "    # KL散度计算: p_teacher * (log p_teacher - log p_mixed)\n",
    "    kl_divergence = probs_teacher * (log_probs_teacher - mixed_log_probs)\n",
    "    kl_divergence = kl_divergence.sum(dim=-1) # 在词汇表维度上求和\n",
    "\n",
    "    # 处理padding\n",
    "    if target_labels is not None and padding_id is not None:\n",
    "        pad_mask = (target_labels == padding_id)\n",
    "        kl_divergence.masked_fill_(pad_mask, 0.0)\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        kl_loss = kl_divergence.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        if target_labels is not None and padding_id is not None:\n",
    "            num_tokens = (target_labels != padding_id).sum()\n",
    "            kl_loss = kl_divergence.sum() / num_tokens if num_tokens > 0 else torch.tensor(0.0).to(kl_divergence.device)\n",
    "        else:\n",
    "            kl_loss = kl_divergence.mean()\n",
    "    else:\n",
    "        kl_loss = kl_divergence\n",
    "\n",
    "    return kl_loss\n"
   ],
   "id": "b0b190e317ffcd56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. 定义KDTrainer (知识蒸馏训练器)\n",
    "class KDTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, use_ce_loss=True,\n",
    "                 kl_loss_weight=0.5, skew_lambda_fkl=0.1, # MODIFIED: Added skew_lambda_fkl\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.use_ce_loss = use_ce_loss\n",
    "        self.kl_loss_weight = kl_loss_weight\n",
    "        self.skew_lambda_fkl = skew_lambda_fkl # MODIFIED: Store skew_lambda for skewed_fkl\n",
    "        if self.teacher_model is not None:\n",
    "            self.teacher_model.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce_student = outputs_student.loss\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        if logits_student.shape[-1] != logits_teacher.shape[-1]:\n",
    "            vocab_size_student = logits_student.shape[-1]\n",
    "            logits_teacher = logits_teacher[..., :vocab_size_student]\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # 计算偏向前KL散度损失\n",
    "        kl_loss = compute_skewed_fkl( # MODIFIED: Changed to compute_skewed_fkl\n",
    "            logits_student,\n",
    "            logits_teacher,\n",
    "            target_labels=labels,\n",
    "            padding_id=self.label_pad_token_id if hasattr(self, 'label_pad_token_id') else -100,\n",
    "            temp=2.0,\n",
    "            reduction=\"sum\",\n",
    "            skew_lambda=self.skew_lambda_fkl # MODIFIED: Pass skew_lambda\n",
    "        )\n",
    "\n",
    "        if self.use_ce_loss:\n",
    "            total_loss = self.kl_loss_weight * kl_loss + (1 - self.kl_loss_weight) * loss_ce_student\n",
    "        else:\n",
    "            total_loss = kl_loss\n",
    "\n",
    "        return (total_loss, outputs_student) if return_outputs else total_loss\n"
   ],
   "id": "348a2fee6a678bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. 配置参数\n",
    "# 模型和路径\n",
    "teacher_model_path = \"qwen_teacher_finetune\"\n",
    "student_model_name = \"unsloth/Qwen2.5-0.5B\"\n",
    "output_dir_distillation = \"./results_qwen_student_distilled_skewed_fkl\" # MODIFIED\n",
    "save_directory_student = \"qwen_student_distilled_skewed_fkl_final\" # MODIFIED\n",
    "\n",
    "# 数据集和格式化\n",
    "dataset_name = \"yahma/alpaca-cleaned\"\n",
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with\n",
    "an input that provides further context. Write a response that appropriately\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# 训练超参数\n",
    "max_seq_length = 1024\n",
    "load_in_4bit = True\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    load_in_4bit = False\n",
    "    dtype = torch.float16\n",
    "    print(\"MPS detected. Disabling 4-bit quantization and using float16.\")\n",
    "else:\n",
    "    dtype = None\n",
    "    print(\"CUDA or CPU detected. Using auto dtype and 4-bit quantization if enabled.\")\n",
    "\n"
   ],
   "id": "ab7b2b43a8e11b48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 蒸馏特定参数\n",
    "distill_use_ce_loss = True\n",
    "distill_kl_loss_weight = 0.5\n",
    "distill_skew_lambda_fkl_value = 0.1 # MODIFIED: Added skew_lambda configuration\n",
    "distill_epochs = 3\n",
    "distill_batch_size = 2\n",
    "distill_grad_accum = 8\n",
    "distill_lr = 5e-5"
   ],
   "id": "8af9b822cefe8e6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. 加载数据集和预处理\n",
    "print(\"Loading and formatting dataset...\")\n",
    "dataset_full = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset_full\n",
    "\n",
    "tokenizer_for_formatting = FastLanguageModel.get_tokenizer(student_model_name)\n",
    "EOS_TOKEN = tokenizer_for_formatting.eos_token\n",
    "if EOS_TOKEN is None:\n",
    "    tokenizer_for_formatting.eos_token = \"<|endoftext|>\"\n",
    "    EOS_TOKEN = tokenizer_for_formatting.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        text = alpaca_prompt_template.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=4)\n",
    "print(f\"Dataset formatted. Number of examples: {len(dataset)}\")\n"
   ],
   "id": "85a78fe245cf59cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. 加载教师模型 (已微调)\n",
    "print(f\"Loading fine-tuned teacher model from {teacher_model_path}...\")\n",
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=teacher_model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(teacher_model)\n",
    "print(\"Teacher model loaded.\")\n"
   ],
   "id": "aba4d8c343419c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6. 加载学生模型并配置LoRA\n",
    "print(f\"Loading student model ({student_model_name}) and configuring LoRA...\")\n",
    "student_model, student_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=student_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"Student model loaded and LoRA configured.\")\n",
    "student_model.print_trainable_parameters()\n",
    "\n",
    "if student_tokenizer.eos_token is None:\n",
    "    student_tokenizer.eos_token = EOS_TOKEN\n",
    "if teacher_tokenizer.eos_token is None:\n",
    "    teacher_tokenizer.eos_token = EOS_TOKEN\n",
    "\n"
   ],
   "id": "c19d16a2bb9bdd43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7. 配置蒸馏训练参数\n",
    "print(\"Configuring TrainingArguments for distillation...\")\n",
    "distill_training_args = TrainingArguments(\n",
    "    output_dir=output_dir_distillation,\n",
    "    num_train_epochs=distill_epochs,\n",
    "    per_device_train_batch_size=distill_batch_size,\n",
    "    gradient_accumulation_steps=distill_grad_accum,\n",
    "    learning_rate=distill_lr,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=not is_bfloat16_supported() and not torch.backends.mps.is_available(),\n",
    "    bf16=is_bfloat16_supported() and not torch.backends.mps.is_available(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ],
   "id": "3b20de69062f44e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 8. 初始化KDTrainer并开始训练\n",
    "print(\"Initializing KDTrainer...\")\n",
    "distill_trainer = KDTrainer(\n",
    "    model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=distill_training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=student_tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    use_ce_loss=distill_use_ce_loss,\n",
    "    kl_loss_weight=distill_kl_loss_weight,\n",
    "    skew_lambda_fkl=distill_skew_lambda_fkl_value # MODIFIED: Pass configured skew_lambda\n",
    ")\n",
    "\n",
    "print(\"Starting distillation training with Skewed Forward KL Divergence...\") # MODIFIED\n",
    "distill_trainer.train()\n",
    "print(\"Distillation training completed.\")\n"
   ],
   "id": "59bc27422ad55170"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 9. 保存蒸馏后的学生模型\n",
    "print(f\"Saving distilled student model to {save_directory_student}...\")\n",
    "student_model.save_pretrained(save_directory_student)\n",
    "student_tokenizer.save_pretrained(save_directory_student)\n",
    "print(\"Distilled student model saved.\")\n",
    "\n",
    "print(\"\\nKnowledge distillation process (Skewed Forward KL) finished.\") # MODIFIED"
   ],
   "id": "5300a5fe5a714c2f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
