{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 安装和导入必要的库\n",
    "!pip install unsloth\n",
    "!pip install git+https://github.com/josejg/instruction_following_eval.git # 安装 IFEval，进行指令跟随评估\n",
    "!pip install -U wandb # 确保 wandb 已安装"
   ],
   "id": "5aa748accce6f6a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import wandb # 导入 wandb\n",
    "import os    # 导入 os\n"
   ],
   "id": "115df4fbfb94516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 0. W&B 设置 (与教师模型脚本类似)\n",
    "# 假设您在 Kaggle 环境中，并且 WANDB_API_KEY 存储在 secrets 中\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    os.environ[\"WANDB_API_KEY\"] = key\n",
    "except ImportError:\n",
    "    print(\"Kaggle secrets not found. Make sure WANDB_API_KEY is set in your environment if not on Kaggle.\")\n",
    "    # 或者直接在这里设置 key = \"YOUR_WANDB_API_KEY\"\n",
    "\n",
    "# 设置 W&B 项目名称 (可以与教师模型项目相同或不同)\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Decoder_Knowledge_Distillation\" # 或者您选择的其他项目名\n",
    "\n",
    "wandb.login()\n"
   ],
   "id": "c4b1f271e0dd4dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 定义KL散度计算函数 (这里是偏向反KL散度)\n",
    "def compute_skewed_rkl(logits_student, logits_teacher, target_labels, padding_id,\n",
    "                       reduction=\"sum\", temp=1.0, skew_lambda=0.1):\n",
    "    \"\"\"计算偏向反KL散度: KL(student || mixed_distribution)\n",
    "       mixed_distribution = (1-skew_lambda) * teacher + skew_lambda * student\n",
    "    \"\"\"\n",
    "    # 温度缩放\n",
    "    logits_student_scaled = logits_student / temp\n",
    "    logits_teacher_scaled = logits_teacher / temp\n",
    "\n",
    "    # 学生模型的概率和对数概率 (来自缩放后的logits)\n",
    "    probs_student = torch.softmax(logits_student_scaled, dim=-1, dtype=torch.float32)\n",
    "    log_probs_student = torch.log_softmax(logits_student_scaled, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # 教师模型的概率 (来自缩放后的logits, 不应反向传播梯度)\n",
    "    with torch.no_grad():\n",
    "        probs_teacher = torch.softmax(logits_teacher_scaled, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # 计算混合概率分布\n",
    "    # mixed_probs = (1 - skew_lambda) * p_teacher + skew_lambda * p_student\n",
    "    mixed_probs = (1 - skew_lambda) * probs_teacher + skew_lambda * probs_student\n",
    "    # 防止 mixed_probs 为0导致log(0)数值问题，添加一个极小值\n",
    "    mixed_log_probs = torch.log(mixed_probs + 1e-10)\n",
    "\n",
    "    # KL散度计算: p_student * (log p_student - log p_mixed)\n",
    "    kl_divergence = probs_student * (log_probs_student - mixed_log_probs)\n",
    "    kl_divergence = kl_divergence.sum(dim=-1) # 在词汇表维度上求和\n",
    "\n",
    "    # 处理padding\n",
    "    if target_labels is not None and padding_id is not None:\n",
    "        pad_mask = (target_labels == padding_id)\n",
    "        kl_divergence.masked_fill_(pad_mask, 0.0)\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        kl_loss = kl_divergence.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        if target_labels is not None and padding_id is not None:\n",
    "            num_tokens = (target_labels != padding_id).sum()\n",
    "            kl_loss = kl_divergence.sum() / num_tokens if num_tokens > 0 else torch.tensor(0.0).to(kl_divergence.device)\n",
    "        else:\n",
    "            kl_loss = kl_divergence.mean()\n",
    "    else:\n",
    "        kl_loss = kl_divergence\n",
    "\n",
    "    return kl_loss\n",
    "\n"
   ],
   "id": "51fd0aab39615dbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class KDTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, use_ce_loss=True,\n",
    "                 kl_loss_weight=0.5, skew_lambda_rkl=0.1,kl_temperature=2.0,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.use_ce_loss = use_ce_loss\n",
    "        self.kl_loss_weight = kl_loss_weight\n",
    "        self.kl_temperature = kl_temperature   # <--- 保存为实例属性\n",
    "        self.skew_lambda_rkl = skew_lambda_rkl\n",
    "        if self.teacher_model is not None:\n",
    "            self.teacher_model.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce_student = outputs_student.loss\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        if logits_student.shape[-1] != logits_teacher.shape[-1]:\n",
    "            vocab_size_student = logits_student.shape[-1]\n",
    "            logits_teacher = logits_teacher[..., :vocab_size_student]\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "        if self.processing_class is not None and hasattr(self.processing_class, \"pad_token_id\"):\n",
    "            padding_id_val = self.processing_class.pad_token_id\n",
    "        else:\n",
    "            padding_id_val = -100\n",
    "        # 计算偏向反KL散度损失\n",
    "        kl_loss = compute_skewed_rkl( # MODIFIED: Changed to compute_skewed_rkl\n",
    "            logits_student,\n",
    "            logits_teacher,\n",
    "            target_labels=labels,\n",
    "            padding_id=padding_id_val,\n",
    "            temp=2.0,\n",
    "            reduction=\"sum\",\n",
    "            skew_lambda=self.skew_lambda_rkl # MODIFIED: Pass skew_lambda\n",
    "        )\n",
    "\n",
    "        if self.use_ce_loss:\n",
    "            total_loss = self.kl_loss_weight * kl_loss + (1 - self.kl_loss_weight) * loss_ce_student\n",
    "        else:\n",
    "            total_loss = kl_loss\n",
    "\n",
    "        return (total_loss, outputs_student) if return_outputs else total_loss"
   ],
   "id": "d1fe0f21c6efeafc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. 配置参数\n",
    "# 模型和路径\n",
    "# teacher_model_path = \"qwen_teacher_finetune\"\n",
    "teacher_model_path = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "student_model_name = \"unsloth/Qwen2.5-0.5B-Instruct\" # 学生模型是 Instruct 模型\n",
    "output_dir_distillation = \"./results_qwen_student_distilled_skewed_rkl_chat\"\n",
    "save_directory_student = \"qwen_student_distilled_skewed_rkl_chat_final\"\n",
    "\n",
    "# 数据集和格式化\n",
    "dataset_name = \"yahma/alpaca-cleaned\"\n",
    "ALPACA_SYSTEM_PROMPT = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "\n",
    "# 训练超参数\n",
    "max_seq_length = 2048\n",
    "load_in_4bit = True\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    load_in_4bit = False\n",
    "    dtype = torch.float16\n",
    "    print(\"MPS detected. Disabling 4-bit quantization and using float16.\")\n",
    "else:\n",
    "    dtype = None\n",
    "    print(\"CUDA or CPU detected. Using auto dtype and 4-bit quantization if enabled.\")\n"
   ],
   "id": "dcfea58636c0c4e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 蒸馏特定参数\n",
    "distill_use_ce_loss = True\n",
    "distill_kl_loss_weight = 0.5\n",
    "distill_epochs = 1\n",
    "distill_batch_size = 2 # 调整以适应显存\n",
    "distill_grad_accum = 8 # Effective batch size = 16\n",
    "distill_lr = 5e-4\n",
    "distill_kl_temperature = 2.0\n",
    "skew_lambda_rkl = 0.1\n",
    "wandb_run_name = f\"decoder_knowledge_distillation_student_skewed_rkl\"\n"
   ],
   "id": "e7a4a032de0cee28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. 加载数据集和预处理\n",
    "print(\"Loading and formatting dataset...\")\n",
    "dataset_full = load_dataset(dataset_name, split=\"train\")\n",
    "# dataset = dataset_full.select(range(200)) # 演示用\n",
    "dataset = dataset_full\n",
    "\n",
    "print(f\"Loading student model ({student_model_name}) and its tokenizer...\")\n",
    "student_model, student_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=student_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit, # 实际加载模型权重\n",
    ")\n",
    "\n",
    "# 确保学生tokenizer有必要的token和chat_template (原第6步的检查)\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "    print(f\"Set student_tokenizer.pad_token to eos_token: {student_tokenizer.pad_token}\")\n",
    "\n",
    "if student_tokenizer.chat_template is None:\n",
    "    print(f\"Warning: student_tokenizer (for {student_model_name}) loaded without a chat_template. Unsloth might apply a default one for Qwen models. Ensure this is intended.\")\n",
    "else:\n",
    "    print(f\"Using student tokenizer chat template: {student_tokenizer.chat_template}\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": ALPACA_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": instruction + (f\"\\n{input_text}\" if input_text and input_text.strip() else \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": output}\n",
    "        ]\n",
    "        try:\n",
    "            # 直接使用 student_tokenizer\n",
    "            formatted_text = student_tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(formatted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying chat template: {e}\")\n",
    "            print(f\"Problematic messages: {messages}\")\n",
    "            texts.append(\"\")\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=4)\n",
    "dataset = dataset.filter(lambda example: example['text'] != \"\" and example['text'] is not None)\n",
    "print(f\"Dataset formatted. Number of examples after formatting: {len(dataset)}\")\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nSample formatted text (for student model training):\")\n",
    "    print(dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Dataset is empty after formatting. Exiting.\")\n",
    "    exit()\n"
   ],
   "id": "65b612b62a2f562a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# #挑选token数前32的数据用于估计显存占用\n",
    "# def count_tokens(example):\n",
    "#     return {\n",
    "#         \"num_tokens\": len(\n",
    "#             student_tokenizer(example[\"text\"], add_special_tokens=False).input_ids\n",
    "#         )\n",
    "#     }\n",
    "\n",
    "# dataset_with_counts = dataset.map(count_tokens, batched=False)\n",
    "\n",
    "# # 2. 按 token 数降序排序\n",
    "# sorted_dataset = dataset_with_counts.sort(\"num_tokens\", reverse=True)\n",
    "\n",
    "# # 3. 取前16条\n",
    "# dataset = sorted_dataset.select(range(32))\n",
    "\n",
    "# dataset ['num_tokens']"
   ],
   "id": "2d9dcc1b074f65ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. 加载教师模型 (已微调)\n",
    "print(f\"Loading fine-tuned teacher model from {teacher_model_path}...\")\n",
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=teacher_model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(teacher_model)\n",
    "print(\"Teacher model loaded.\")\n",
    "\n",
    "# 确保教师tokenizer有必要的token，以防万一\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "    print(f\"Set teacher_tokenizer.pad_token to eos_token: {teacher_tokenizer.pad_token}\")\n"
   ],
   "id": "c2d855f5a4bfe63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6. 学生模型配置LoRA\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "print(\"Student model loaded and LoRA configured.\")\n",
    "student_model.print_trainable_parameters()\n"
   ],
   "id": "42acc51e73d1fbfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7. 配置蒸馏训练参数\n",
    "print(\"Configuring TrainingArguments for distillation...\")\n",
    "distill_training_args = TrainingArguments(\n",
    "    output_dir=output_dir_distillation,\n",
    "    num_train_epochs=distill_epochs,\n",
    "    # max_steps=1, # 如果使用max_steps\n",
    "    per_device_train_batch_size=distill_batch_size,\n",
    "    gradient_accumulation_steps=distill_grad_accum,\n",
    "    learning_rate=distill_lr,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10, # 调整日志频率\n",
    "    save_strategy=\"epoch\", # 或 \"steps\"\n",
    "    # save_steps=50, # 如果 save_strategy=\"steps\"\n",
    "    save_total_limit=2,\n",
    "    fp16=not is_bfloat16_supported() and not torch.backends.mps.is_available(),\n",
    "    bf16=is_bfloat16_supported() and not torch.backends.mps.is_available(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"wandb\", # <--- 修改这里以启用W&B报告\n",
    "    run_name=wandb_run_name, # <--- 为W&B运行设置名称\n",
    ")\n"
   ],
   "id": "91d9bd037d24830a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 8. 初始化KDTrainer并开始训练\n",
    "if len(dataset) == 0:\n",
    "    print(\"Skipping distillation training as dataset is empty.\")\n",
    "else:\n",
    "    print(\"Initializing KDTrainer...\")\n",
    "    distill_trainer = KDTrainer(\n",
    "        model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        args=distill_training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=student_tokenizer, # KDTrainer 使用学生 tokenizer\n",
    "        dataset_text_field=\"text\",   # 我们在 formatting_prompts_func 中创建了这个字段\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False, # 因为 'text' 字段是预格式化的完整对话\n",
    "        use_ce_loss=distill_use_ce_loss,\n",
    "        kl_loss_weight=distill_kl_loss_weight,\n",
    "        kl_temperature = distill_kl_temperature,\n",
    "        skew_lambda_rkl = skew_lambda_rkl,\n",
    "    )\n",
    "\n",
    "    print(\"Starting distillation training with Forward KL Divergence and Chat Template...\")\n",
    "    distill_trainer.train()\n",
    "    wandb.finish()\n",
    "    print(\"Distillation training completed.\")\n",
    "\n",
    "    # 9. 保存蒸馏后的学生模型 (LoRA权重) 和分词器\n",
    "    print(f\"Saving distilled student model to {save_directory_student}...\")\n",
    "    student_model.save_pretrained(save_directory_student)\n",
    "    student_tokenizer.save_pretrained(save_directory_student)\n",
    "    print(\"Distilled student model saved.\")\n",
    "\n",
    "print(\"\\nKnowledge distillation process (Forward KL with Chat Template) finished.\")"
   ],
   "id": "355749186a4ff8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# IFEval Evaluation for the Distilled Student Model\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\nStarting IFEval Evaluation for the distilled student model...\")\n",
    "\n",
    "try:\n",
    "    from instruction_following_eval import get_examples, evaluate_instruction_following\n",
    "except ImportError:\n",
    "    print(\"IFEval library not found. Please install it first.\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(save_directory_student) or not os.listdir(save_directory_student): # 检查目录是否存在且不为空\n",
    "    print(f\"Error: Saved model directory '{save_directory_student}' not found or empty. Skipping IFEval.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading distilled student model from {save_directory_student} for IFEval...\")\n",
    "eval_model, eval_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=save_directory_student,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(eval_model)\n",
    "print(\"Distilled student model and tokenizer loaded for IFEval.\")"
   ],
   "id": "af9a66e3f004c0d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if eval_tokenizer.pad_token is None:\n",
    "    eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "    print(f\"Set eval_tokenizer.pad_token to eos_token: {eval_tokenizer.pad_token}\")\n",
    "\n",
    "# 确保评估时使用的tokenizer也有正确的chat_template\n",
    "# 通常从保存的目录加载时，它会包含训练时的配置\n",
    "if eval_tokenizer.chat_template is None:\n",
    "    print(f\"Warning: eval_tokenizer (for {save_directory_student}) loaded without a chat_template.\")\n",
    "    if student_tokenizer.chat_template is not None: # student_tokenizer 是训练时用的\n",
    "        eval_tokenizer.chat_template = student_tokenizer.chat_template\n",
    "        print(f\"Applied chat_template from student_tokenizer to eval_tokenizer.\")\n",
    "    # 如果 student_tokenizer 也没有，那可能需要手动设置或依赖模型默认行为\n",
    "else:\n",
    "    print(f\"Eval tokenizer chat template: {eval_tokenizer.chat_template}\")"
   ],
   "id": "62ae956f8dd57063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ifeval_examples = get_examples()\n",
    "print(f\"Loaded {len(ifeval_examples)} examples for IFEval.\")\n",
    "# ifeval_examples = ifeval_examples[:5] # 演示用\n",
    "\n",
    "print(\"Generating responses for IFEval prompts using the distilled student model...\")\n",
    "generated_responses_for_ifeval = [] # IFEval期望一个包含'response'键的字典列表\n",
    "\n",
    "for i, example in enumerate(ifeval_examples):\n",
    "    ifeval_prompt_text = example['prompt']\n",
    "    messages_for_eval = [\n",
    "        {\"role\": \"system\", \"content\": ALPACA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": ifeval_prompt_text}\n",
    "    ]\n",
    "    try:\n",
    "        inputs = eval_tokenizer.apply_chat_template(\n",
    "            messages_for_eval,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True, # 重要: 为生成任务设为True\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(eval_model.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying chat template for IFEval prompt: {e}\")\n",
    "        example['response'] = f\"Error during input formatting: {e}\"\n",
    "        generated_responses_for_ifeval.append(example)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        outputs = eval_model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=2048, # 调整最大生成长度\n",
    "            use_cache=True\n",
    "        )\n",
    "        response_text = eval_tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model generation for IFEval prompt {i+1}: {e}\")\n",
    "        response_text = f\"Error during model generation: {e}\"\n",
    "\n",
    "    current_example_with_response = example.copy() # 复制原始字典\n",
    "    current_example_with_response['response'] = response_text # 添加 'response' 键\n",
    "    generated_responses_for_ifeval.append(current_example_with_response)\n",
    "\n",
    "    if (i + 1) % 10 == 0 or i == len(ifeval_examples) - 1:\n",
    "        print(f\"Generated response for IFEval example {i + 1}/{len(ifeval_examples)}\")\n",
    "\n",
    "print(\"Finished generating responses for IFEval prompts.\")\n",
    "\n",
    "if generated_responses_for_ifeval:\n",
    "    print(\"Evaluating generated responses with IFEval...\")\n",
    "    model_responses_list = [ex['response'] for ex in generated_responses_for_ifeval]\n",
    "    ifeval_metrics = evaluate_instruction_following(ifeval_examples, model_responses_list) # 使用原始ifeval_examples和提取的responses\n",
    "\n",
    "    print(\"\\nIFEval Metrics for Distilled Student Model:\")\n",
    "    for metric_name, value in ifeval_metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"No responses were generated, skipping IFEval evaluation.\")\n",
    "\n",
    "print(\"\\nIFEval Evaluation for distilled student model finished.\")"
   ],
   "id": "c9ba4acb1f2c4e01"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
