{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 安装和导入必要的库\n",
    "!pip install  unsloth"
   ],
   "id": "68de193a990655b8",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n"
   ],
   "id": "edd6759fb3410286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 定义KL散度计算函数 (这里只包含反向KL散度)\n",
    "def compute_rkl(logits_student, logits_teacher, target_labels, padding_id, reduction=\"sum\", temp=1.0):\n",
    "    \"\"\"计算反向KL散度: KL(student || teacher)\"\"\"\n",
    "    # 温度缩放\n",
    "    logits_student = logits_student / temp\n",
    "    logits_teacher = logits_teacher / temp\n",
    "\n",
    "    # 计算学生模型的概率和对数概率\n",
    "    probs_student = torch.softmax(logits_student, dim=-1, dtype=torch.float32)\n",
    "    log_probs_student = torch.log_softmax(logits_student, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # 计算教师模型的对数概率 (教师模型不应反向传播梯度)\n",
    "    with torch.no_grad():\n",
    "        log_probs_teacher = torch.log_softmax(logits_teacher, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # KL散度计算: q * (log q - log p) = q * log(q/p)\n",
    "    kl_divergence = probs_student * (log_probs_student - log_probs_teacher)\n",
    "    kl_divergence = kl_divergence.sum(dim=-1) # 在词汇表维度上求和\n",
    "\n",
    "    # 处理padding\n",
    "    if target_labels is not None and padding_id is not None:\n",
    "        pad_mask = (target_labels == padding_id)\n",
    "        kl_divergence.masked_fill_(pad_mask, 0.0)\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        kl_loss = kl_divergence.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        # 计算有效的 (非padding) token数量\n",
    "        if target_labels is not None and padding_id is not None:\n",
    "            num_tokens = (target_labels != padding_id).sum()\n",
    "            kl_loss = kl_divergence.sum() / num_tokens if num_tokens > 0 else torch.tensor(0.0).to(kl_divergence.device)\n",
    "        else: # 如果没有提供target_labels或padding_id，则对所有token取平均\n",
    "            kl_loss = kl_divergence.mean()\n",
    "    else:\n",
    "        kl_loss = kl_divergence # 返回每个token的KL散度值\n",
    "\n",
    "    return kl_loss\n"
   ],
   "id": "8c0c2f95b78d6ae6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. 定义KDTrainer (知识蒸馏训练器)\n",
    "class KDTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, use_ce_loss=True, kl_loss_weight=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.use_ce_loss = use_ce_loss # 是否使用学生模型的交叉熵损失\n",
    "        self.kl_loss_weight = kl_loss_weight # KL散度损失的权重\n",
    "        if self.teacher_model is not None:\n",
    "            self.teacher_model.eval() # 确保教师模型在评估模式\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # model 是学生模型\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce_student = outputs_student.loss # 学生模型对硬标签的原始交叉熵损失\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        # 确保logits形状兼容 (简化处理：以学生词汇表为准截断教师)\n",
    "        if logits_student.shape[-1] != logits_teacher.shape[-1]:\n",
    "            vocab_size_student = logits_student.shape[-1]\n",
    "            logits_teacher = logits_teacher[..., :vocab_size_student]\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # 计算反向KL散度损失 (使用上面定义的compute_rkl)\n",
    "        # SFTTrainer通常使用-100作为忽略标签的padding_id\n",
    "        # 温度参数可以调整，这里使用2.0作为示例\n",
    "        kl_loss = compute_rkl(\n",
    "            logits_student,\n",
    "            logits_teacher,\n",
    "            target_labels=labels,\n",
    "            padding_id=self.label_pad_token_id if hasattr(self, 'label_pad_token_id') else -100, # 从SFTTrainer获取\n",
    "            temp=2.0,\n",
    "            reduction=\"sum\" # 或者 \"mean\"，取决于你的偏好和loss_ce的reduction方式\n",
    "        )\n",
    "\n",
    "        # 如果loss_ce_student是每个样本的平均损失，kl_loss也应该做相应调整\n",
    "        # SFTTrainer的loss通常是batch内所有token损失的平均或总和，这里假设是总和或可比的平均\n",
    "        # 为了简单，我们假设kl_loss和loss_ce_student的reduction方式是兼容的\n",
    "        # 如果loss_ce_student是平均值，kl_loss也应该用reduction=\"mean\"或手动平均\n",
    "\n",
    "        if self.use_ce_loss:\n",
    "            # 总损失 = KL散度损失 + 交叉熵损失\n",
    "            # 权重可以调整，例如 kl_loss_weight 控制KL散度的重要性\n",
    "            total_loss = self.kl_loss_weight * kl_loss + (1 - self.kl_loss_weight) * loss_ce_student\n",
    "        else:\n",
    "            # 只使用KL散度损失\n",
    "            total_loss = kl_loss\n",
    "\n",
    "        return (total_loss, outputs_student) if return_outputs else total_loss\n"
   ],
   "id": "8476655103e0bded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3. 配置参数\n",
    "# 模型和路径\n",
    "teacher_model_path = \"qwen_teacher_finetune\" # 假设微调好的教师模型在此\n",
    "student_model_name = \"unsloth/Qwen2.5-0.5B\" # 选择一个更小的学生模型，例如0.5B\n",
    "# student_model_name = \"unsloth/Qwen2.5-1.5B\" # 或者1.5B\n",
    "output_dir_distillation = \"./results_qwen_student_distilled_rkl\"\n",
    "save_directory_student = \"qwen_student_distilled_rkl_final\"\n",
    "\n",
    "# 数据集和格式化\n",
    "dataset_name = \"yahma/alpaca-cleaned\"\n",
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with\n",
    "an input that provides further context. Write a response that appropriately\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# 训练超参数\n",
    "max_seq_length = 1024 # 可以根据需要调整，但要小于等于模型支持的最大长度\n",
    "load_in_4bit = True\n",
    "# dtype = None # for auto-detection\n",
    "# For Apple Silicon (MPS), 4-bit quantization might not be supported by unsloth or bitsandbytes\n",
    "# In that case, set load_in_4bit = False and potentially dtype = torch.float16\n",
    "if torch.backends.mps.is_available():\n",
    "    load_in_4bit = False # MPS 通常不支持4位量化\n",
    "    dtype = torch.float16\n",
    "    print(\"MPS detected. Disabling 4-bit quantization and using float16.\")\n",
    "else:\n",
    "    dtype = None # Auto detection for CUDA\n",
    "    print(\"CUDA or CPU detected. Using auto dtype and 4-bit quantization if enabled.\")\n",
    "\n"
   ],
   "id": "f47b942a5065b830",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 蒸馏特定参数\n",
    "distill_use_ce_loss = True # 是否在蒸馏时也使用学生模型的CE损失\n",
    "distill_kl_loss_weight = 0.5 # 如果使用CE损失，KL散度损失的权重 (CE损失权重为 1 - kl_loss_weight)\n",
    "distill_epochs = 3 # 减少epoch以便快速演示\n",
    "distill_batch_size = 2\n",
    "distill_grad_accum = 8\n",
    "distill_lr = 5e-5 # 调整学习率"
   ],
   "id": "8a37876f31b924b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. 加载数据集和预处理\n",
    "print(\"Loading and formatting dataset...\")\n",
    "dataset_full = load_dataset(dataset_name, split=\"train\")\n",
    "# dataset = dataset_full.select(range(2000)) # 可选：为了快速演示，使用部分数据\n",
    "dataset = dataset_full\n",
    "\n",
    "tokenizer_for_formatting = FastLanguageModel.get_tokenizer(student_model_name) # 获取一个分词器用于格式化\n",
    "EOS_TOKEN = tokenizer_for_formatting.eos_token\n",
    "if EOS_TOKEN is None:\n",
    "    tokenizer_for_formatting.eos_token = \"<|endoftext|>\" # 确保有EOS token\n",
    "    EOS_TOKEN = tokenizer_for_formatting.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        text = alpaca_prompt_template.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=4)\n",
    "print(f\"Dataset formatted. Number of examples: {len(dataset)}\")\n"
   ],
   "id": "e9bfd85a5f3038b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. 加载教师模型 (已微调)\n",
    "print(f\"Loading fine-tuned teacher model from {teacher_model_path}...\")\n",
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=teacher_model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(teacher_model) # 设置为推理模式\n",
    "print(\"Teacher model loaded.\")\n"
   ],
   "id": "9155123670078f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6. 加载学生模型并配置LoRA\n",
    "print(f\"Loading student model ({student_model_name}) and configuring LoRA...\")\n",
    "student_model, student_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=student_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "student_model = FastLanguageModel.get_peft_model(\n",
    "    student_model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"Student model loaded and LoRA configured.\")\n",
    "student_model.print_trainable_parameters()\n",
    "\n",
    "# 确保学生和教师的分词器eos_token一致，如果之前没有设置的话\n",
    "if student_tokenizer.eos_token is None:\n",
    "    student_tokenizer.eos_token = EOS_TOKEN # 使用之前为格式化定义的EOS_TOKEN\n",
    "if teacher_tokenizer.eos_token is None:\n",
    "    teacher_tokenizer.eos_token = EOS_TOKEN\n",
    "\n"
   ],
   "id": "5ce477b2df796076"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7. 配置蒸馏训练参数\n",
    "print(\"Configuring TrainingArguments for distillation...\")\n",
    "distill_training_args = TrainingArguments(\n",
    "    output_dir=output_dir_distillation,\n",
    "    # max_steps=1,       # 或者按max_steps训练\n",
    "    num_train_epochs=distill_epochs,\n",
    "    per_device_train_batch_size=distill_batch_size,\n",
    "    gradient_accumulation_steps=distill_grad_accum,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=not is_bfloat16_supported() and not torch.backends.mps.is_available(), # fp16 if not bf16 and not mps\n",
    "    bf16=is_bfloat16_supported() and not torch.backends.mps.is_available(),    # bf16 if supported and not mps\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ],
   "id": "14716e179385a0b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 8. 初始化KDTrainer并开始训练\n",
    "print(\"Initializing KDTrainer...\")\n",
    "distill_trainer = KDTrainer(\n",
    "    model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=distill_training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=student_tokenizer, # 使用学生模型的分词器\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    use_ce_loss=distill_use_ce_loss,\n",
    "    kl_loss_weight=distill_kl_loss_weight,\n",
    ")\n",
    "\n",
    "print(\"Starting distillation training with Reverse KL Divergence...\")\n",
    "distill_trainer.train() # resume_from_checkpoint=False by default\n",
    "print(\"Distillation training completed.\")\n"
   ],
   "id": "782aefb467c4cf67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 9. 保存蒸馏后的学生模型\n",
    "print(f\"Saving distilled student model to {save_directory_student}...\")\n",
    "student_model.save_pretrained(save_directory_student)\n",
    "student_tokenizer.save_pretrained(save_directory_student)\n",
    "print(\"Distilled student model saved.\")\n",
    "\n",
    "print(\"\\nKnowledge distillation process finished.\")"
   ],
   "id": "944879c2c9c8029"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
