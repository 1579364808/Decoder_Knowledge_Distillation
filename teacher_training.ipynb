{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732f62dd9bb79b1",
   "metadata": {},
   "source": "# !pip install unsloth",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with\n",
    "an input that provides further context. Write a response that appropriately\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "dataset_full = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "# 为了快速演示，可以只取一小部分数据\n",
    "# dataset = dataset_full.select(range(1000)) # 例如，取前1000条\n",
    "dataset = dataset_full # 使用完整数据集\n",
    "\n",
    "print(f\"Dataset loaded. Number of examples: {len(dataset)}\")"
   ],
   "id": "90e495e19db28b95",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None  # Auto detection\n",
    "load_in_4bit = True # 使用4位量化以节省显存\n",
    "\n",
    "# 加载教师模型和分词器\n",
    "teacher_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "if EOS_TOKEN is None: # 有些tokenizer可能没有预设eos_token，需要手动指定或添加\n",
    "    tokenizer.eos_token = \"<|endoftext|>\" # 示例，具体看Qwen2.5的推荐\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    print(f\"EOS_TOKEN was None, set to: {EOS_TOKEN}\")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # alpaca_prompt.format的参数顺序是 instruction, input, output\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts, }\n",
    "\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
    "print(\"Dataset formatted.\")\n"
   ],
   "id": "f651a5604bf8b29b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 配置LoRA参数\n",
    "teacher_model = FastLanguageModel.get_peft_model(\n",
    "    teacher_model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"Teacher model configured with LoRA.\")\n",
    "teacher_model.print_trainable_parameters() # 打印可训练参数信息\n",
    "\n"
   ],
   "id": "164c9177ab24e1cf",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"\\nTraining and saving the teacher model...\")\n",
    "\n",
    "# 配置训练参数\n",
    "training_args_teacher = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4, # Effective batch size = 2 * 4 = 8\n",
    "    # num_train_epochs=1, # 可以按epoch训练\n",
    "    max_steps=1,       # 或者按max_steps训练 (文档中使用此方式)\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs_teacher_finetune\", # 为教师模型微调指定一个清晰的输出目录\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 初始化SFTTrainer\n",
    "trainer_teacher = SFTTrainer(\n",
    "    model=teacher_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2, # 根据你的CPU核心数调整\n",
    "    packing=False,\n",
    "    args=training_args_teacher,\n",
    ")\n",
    "\n",
    "print(\"Starting teacher model fine-tuning...\")\n",
    "trainer_teacher.train()\n",
    "print(\"Teacher model fine-tuning completed.\")\n"
   ],
   "id": "ba7de1dee27c345f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 保存微调后的教师模型 (LoRA权重) 和分词器\n",
    "save_directory_teacher = \"qwen_teacher_finetune\" # 与文档一致的保存目录名\n",
    "teacher_model.save_pretrained(save_directory_teacher)\n",
    "tokenizer.save_pretrained(save_directory_teacher)\n",
    "print(f\"Finetuned teacher model and tokenizer saved to '{save_directory_teacher}'.\")\n",
    "\n",
    "print(\"\\nTeacher model training and saving process finished.\")"
   ],
   "id": "18d50d695a92ff62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
