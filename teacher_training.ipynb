{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install unsloth\n",
    "!pip install git+https://github.com/josejg/instruction_following_eval.git # 安装 IFEval，进行指令跟随评估\n",
    "!pip install -U wandb"
   ],
   "id": "da439afa4920e2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import wandb\n",
    "import os"
   ],
   "id": "1ac77f811296c4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "os.environ[\"WANDB_API_KEY\"]=key\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Decoder_Knowledge_Distillation\""
   ],
   "id": "18a395c0b6a819b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "wandb.login()",
   "id": "afa78e58cc35e5d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ],
   "id": "2f0cecd19cc3290d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载数据集\n",
    "dataset_full = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "# 为了快速演示，可以只取一小部分数据\n",
    "# dataset = dataset_full.select(range(100)) # 例如，取前100条\n",
    "dataset = dataset_full # 使用完整数据集\n",
    "\n",
    "print(f\"Dataset loaded. Number of examples: {len(dataset)}\")\n"
   ],
   "id": "6936f7f51721513d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth.chat_templates import CHAT_TEMPLATES\n",
    "print(list(CHAT_TEMPLATES.keys()))"
   ],
   "id": "a302d495478e87dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 模型和分词器参数\n",
    "max_seq_length = 2048\n",
    "dtype = None  # Auto detection by Unsloth\n",
    "load_in_4bit = True # 使用4位量化以节省显存\n",
    "\n",
    "# 加载模型和分词器\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "# # 检查 tokenizer 是否有 chat_template\n",
    "# if tokenizer.chat_template is None:\n",
    "#     print(\"Warning: tokenizer.chat_template is None. Attempting to proceed, but formatting might be suboptimal if the model expects a specific chat format not applied by default.\")\n",
    "# else:\n",
    "#     print(f\"Using chat template: {tokenizer.chat_template}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Alpaca 的原始 system prompt\n",
    "ALPACA_SYSTEM_PROMPT = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        messages = []\n",
    "        # 1. System Prompt (Alpaca-style)\n",
    "        messages.append({\"role\": \"system\", \"content\": ALPACA_SYSTEM_PROMPT})\n",
    "\n",
    "        # 2. User Prompt (Instruction + Input)\n",
    "        user_content = instruction\n",
    "        if input_text and input_text.strip(): # 如果存在有效的 input\n",
    "            user_content += f\"\\n{input_text}\" # 将 instruction 和 input 合并为用户消息\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # 3. Assistant Response (Output)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "\n",
    "        # 使用 apply_chat_template\n",
    "        # tokenize=False 因为 SFTTrainer 会在内部进行 tokenization\n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False # 重要：因为我们已经提供了完整的助手回复，所以不需要添加生成提示\n",
    "            )\n",
    "            texts.append(formatted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying chat template: {e}\")\n",
    "            print(f\"Problematic messages: {messages}\")\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
    "# 过滤掉处理失败的空字符串样本\n",
    "dataset = dataset.filter(lambda example: example['text'] != \"\")\n",
    "print(f\"Dataset formatted. Number of examples after formatting: {len(dataset)}\")\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nSample formatted text:\")\n",
    "    print(dataset[0]['text'])\n",
    "else:\n",
    "    print(\"Dataset is empty after formatting, please check formatting_prompts_func and chat template.\")\n",
    "    # 如果数据集为空，后续步骤会失败，这里可以提前退出或抛出错误\n",
    "    # exit()\n"
   ],
   "id": "1fde3ebfb3efc6af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# #挑选token数前32的数据用于估计显存占用\n",
    "# def count_tokens(example):\n",
    "#     return {\n",
    "#         \"num_tokens\": len(\n",
    "#             tokenizer(example[\"text\"], add_special_tokens=False).input_ids\n",
    "#         )\n",
    "#     }\n",
    "\n",
    "# dataset_with_counts = dataset.map(count_tokens, batched=False)\n",
    "\n",
    "# # 2. 按 token 数降序排序\n",
    "# sorted_dataset = dataset_with_counts.sort(\"num_tokens\", reverse=True)\n",
    "\n",
    "# # 3. 取前16条\n",
    "# dataset = sorted_dataset.select(range(32))\n",
    "\n",
    "# dataset ['num_tokens']"
   ],
   "id": "1b700b9c14693a34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 配置LoRA参数\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0, # Unsloth 推荐 lora_dropout=0\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\", # 推荐使用 Unsloth 的梯度检查点\n",
    "    random_state=3407,\n",
    "    max_seq_length=max_seq_length, # 确保与模型加载时一致\n",
    ")\n",
    "print(\"Model configured with LoRA.\")\n",
    "model.print_trainable_parameters()\n"
   ],
   "id": "f83ac2ff4690e7e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if len(dataset) == 0:\n",
    "    print(\"Skipping training as dataset is empty.\")\n",
    "else:\n",
    "    print(\"\\nTraining and saving the model...\")\n",
    "\n",
    "    # 配置训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=8, # 根据你的显存调整\n",
    "        gradient_accumulation_steps=2, # Effective batch size = 16\n",
    "        num_train_epochs=1, # 可以按epoch训练\n",
    "        # max_steps=1, # 演示用，\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=2e-4, # 可以尝试 1e-4, 2e-4\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=100,\n",
    "        optim=\"adamw_8bit\", # Unsloth 推荐\n",
    "        weight_decay=0.01, # Unsloth 推荐\n",
    "        lr_scheduler_type=\"linear\", # Unsloth 推荐\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_qwen_teacher_finetune\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"teacher_training\",\n",
    "    )\n",
    "\n",
    "    # 初始化SFTTrainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\", # 我们在 formatting_prompts_func 中创建了这个字段\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=4, # 根据你的CPU核心数调整\n",
    "        packing=False,  # 重要：当 dataset_text_field 包含完整对话时，packing=False\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    print(\"Starting model fine-tuning...\")\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    print(\"Model fine-tuning completed.\")\n",
    "\n",
    "    # %%\n",
    "    # 保存微调后的模型 (LoRA权重) 和分词器\n",
    "    save_directory = \"qwen_teacher_finetune\"\n",
    "    trainer.save_model(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(f\"Finetuned model and tokenizer saved to '{save_directory}'.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ],
   "id": "f98071713aaddee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nStarting IFEval Evaluation...\")\n",
    "\n",
    "# 1. 确保 IFEval 库已安装 (在脚本开头用 !pip install ... )\n",
    "try:\n",
    "    from instruction_following_eval import get_examples, evaluate_instruction_following\n",
    "except ImportError:\n",
    "    print(\"IFEval library not found. Please install it first:\")\n",
    "    print(\"!pip install git+https://github.com/josejg/instruction_following_eval.git\")\n",
    "    exit()"
   ],
   "id": "5fa337f336afbb04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. 加载微调后的模型以进行推理\n",
    "print(f\"Loading fine-tuned model from {save_directory} for IFEval...\")\n",
    "# 检查 save_directory 是否已定义 (例如，如果跳过了训练)\n",
    "if 'save_directory' not in globals() or not save_directory:\n",
    "    print(\"Error: 'save_directory' is not defined. Cannot load model for IFEval.\")\n",
    "    print(\"This might happen if training was skipped and no model was saved.\")\n",
    "    exit()\n",
    "\n",
    "eval_model, eval_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=save_directory,  # 从保存的目录加载 (它会自动找到基础模型并应用adapter)\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,  # 与微调时保持一致或根据评估需求调整\n",
    ")\n",
    "FastLanguageModel.for_inference(eval_model)  # 准备模型进行推理 (Unsloth推荐)\n",
    "print(\"Fine-tuned model and tokenizer loaded for IFEval.\")"
   ],
   "id": "598fc3a6ee41374c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. 获取 IFEval 的评估样本\n",
    "ifeval_examples = get_examples()\n",
    "print(f\"Loaded {len(ifeval_examples)} examples for IFEval.\")\n",
    "\n",
    "# # 为了快速演示，可以只评估一小部分 IFEval 样本\n",
    "# ifeval_examples = ifeval_examples[:1]\n",
    "# print(f\"Using a subset of {len(ifeval_examples)} examples for IFEval demonstration.\")\n"
   ],
   "id": "62c89d488237a8fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. 为每个 IFEval 样本的 `prompt` 生成模型的 `response`\n",
    "print(\"Generating responses for IFEval prompts...\")\n",
    "for i in range(len(ifeval_examples)):\n",
    "    # 通过索引直接获取和修改 ifeval_examples 中的元素\n",
    "    current_example = ifeval_examples[i]\n",
    "    ifeval_prompt_text = current_example['prompt']\n",
    "\n",
    "    # 使用与微调时类似的格式化方式来构建输入给模型\n",
    "    messages_for_eval = [\n",
    "        {\"role\": \"system\", \"content\": ALPACA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": ifeval_prompt_text}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        inputs = eval_tokenizer.apply_chat_template(\n",
    "            messages_for_eval,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(eval_model.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying chat template for IFEval prompt {i+1}: {e}\")\n",
    "        print(f\"Problematic messages: {messages_for_eval}\")\n",
    "        # 确保即使出错，response 字段也被赋值\n",
    "        ifeval_examples[i]['response'] = f\"Error during input formatting: {e}\"\n",
    "        continue # 继续下一个样本\n",
    "\n",
    "    # 生成回复\n",
    "    try:\n",
    "        outputs = eval_model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=2048,  # 调整了最大生成 token 数，2048 可能对某些指令过长，也更耗时\n",
    "            use_cache=True,\n",
    "            pad_token_id=eval_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # 解码生成的文本\n",
    "        response_text = eval_tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
    "        response_text = response_text.strip()\n",
    "\n",
    "        # 关键：通过索引将生成的 response 赋值回 ifeval_examples 列表中的字典\n",
    "        ifeval_examples[i]['response'] = response_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model generation for IFEval prompt {i+1}: {e}\")\n",
    "        # 确保即使生成出错，response 字段也被赋值\n",
    "        ifeval_examples[i]['response'] = f\"Error during model generation: {e}\"\n",
    "        continue # 继续下一个样本\n",
    "\n",
    "\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(ifeval_examples):  # 每10个或最后一个打印进度\n",
    "        print(f\"Generated response for IFEval example {i + 1}/{len(ifeval_examples)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finished generating responses for IFEval prompts.\")\n"
   ],
   "id": "2586b4a42a7ed20f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ifeval_examples\n",
   "id": "98864da6cba558a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_responses = [example['response'] for example in ifeval_examples]\n",
    "\n",
    "# 现在传递两个参数给评估函数\n",
    "ifeval_metrics = evaluate_instruction_following(ifeval_examples, model_responses)"
   ],
   "id": "1826daa976211b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nIFEval Metrics:\")\n",
    "for metric_name, value in ifeval_metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nIFEval Evaluation finished.\")"
   ],
   "id": "3d479d6b9da79b5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
